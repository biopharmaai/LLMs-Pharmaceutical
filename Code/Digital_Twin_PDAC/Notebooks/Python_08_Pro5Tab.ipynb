{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Gryf0DQ86NAq",
        "outputId": "f421fb59-488a-4cd1-862e-01a310185df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Self-Scoring Bias Analysis Table\n",
            "==================================================\n",
            "             Judge  Self Score  Avg Others Score  Bias Score Bias Type\n",
            "A (ChatGPT o3-pro)        9.12              7.78        1.34   Lenient\n",
            " B (Claude Opus 4)        9.12              7.28        1.84   Lenient\n",
            "C (Gemini 2.5 Pro)        6.75              8.29       -1.54     Harsh\n",
            "    D (ChatGPT o3)        8.90              7.64        1.26   Lenient\n",
            "  E (Grok 3 Think)        7.00              8.69       -1.69     Harsh\n",
            "\n",
            "Average Bias: 0.24\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data setup\n",
        "judges = ['A', 'B', 'C', 'D', 'E']\n",
        "criteria = ['Report/MA Cited', 'Deliverable Completion', 'PDAC Trial Impact', 'Funding Probability']\n",
        "\n",
        "# Scoring data [Judge][Proposal][Criteria]\n",
        "scores = {\n",
        "    'A': {\n",
        "        'A': [9.5, 9.5, 9.0, 8.5], 'B': [9.0, 9.0, 8.5, 9.0], 'C': [7.5, 8.0, 7.5, 7.0],\n",
        "        'D': [8.5, 9.0, 9.5, 8.0], 'E': [6.0, 6.5, 6.0, 5.5]\n",
        "    },\n",
        "    'B': {\n",
        "        'A': [8.5, 9.5, 8.5, 8.0], 'B': [9.5, 9.0, 9.5, 8.5], 'C': [7.0, 7.5, 7.0, 6.5],\n",
        "        'D': [8.0, 8.5, 8.0, 7.5], 'E': [5.5, 6.0, 5.5, 5.0]\n",
        "    },\n",
        "    'C': {\n",
        "        'A': [9.5, 10.0, 9.0, 9.5], 'B': [9.0, 9.0, 8.5, 8.0], 'C': [7.0, 8.5, 6.0, 5.5],\n",
        "        'D': [9.8, 9.5, 10.0, 9.8], 'E': [5.0, 6.0, 5.0, 5.0]\n",
        "    },\n",
        "    'D': {\n",
        "        'A': [9.5, 9.5, 9.0, 8.8], 'B': [8.4, 8.3, 8.0, 7.9], 'C': [7.3, 7.6, 7.1, 6.7],\n",
        "        'D': [9.0, 9.0, 9.2, 8.4], 'E': [6.1, 6.4, 6.0, 5.6]\n",
        "    },\n",
        "    'E': {\n",
        "        'A': [9.0, 9.5, 9.0, 8.5], 'B': [9.0, 9.0, 9.0, 9.5], 'C': [8.5, 8.5, 8.5, 8.0],\n",
        "        'D': [8.0, 8.0, 8.0, 9.0], 'E': [7.0, 7.0, 7.0, 7.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Calculate self-scoring bias\n",
        "bias_data = []\n",
        "for judge in judges:\n",
        "    self_score = np.mean(scores[judge][judge])\n",
        "    other_scores = [np.mean(scores[judge][prop]) for prop in judges if prop != judge]\n",
        "    avg_other_score = np.mean(other_scores)\n",
        "    bias = self_score - avg_other_score\n",
        "\n",
        "    bias_data.append({\n",
        "        'Judge': f'{judge} ({[\"ChatGPT o3-pro\", \"Claude Opus 4\", \"Gemini 2.5 Pro\", \"ChatGPT o3\", \"Grok 3 Think\"][ord(judge)-ord(\"A\")]})',\n",
        "        'Self Score': round(self_score, 2),\n",
        "        'Avg Others Score': round(avg_other_score, 2),\n",
        "        'Bias Score': round(bias, 2),\n",
        "        'Bias Type': 'Lenient' if bias > 0.5 else 'Harsh' if bias < -0.5 else 'Neutral'\n",
        "    })\n",
        "\n",
        "df_bias = pd.DataFrame(bias_data)\n",
        "print(\"1. Self-Scoring Bias Analysis Table\")\n",
        "print(\"=\"*50)\n",
        "print(df_bias.to_string(index=False))\n",
        "print(f\"\\nAverage Bias: {df_bias['Bias Score'].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Calculate overall scores for each judge-proposal combination\n",
        "judge_scores = {}\n",
        "for judge in judges:\n",
        "    judge_scores[judge] = []\n",
        "    for prop in ['A', 'B', 'C', 'D', 'E']:\n",
        "        judge_scores[judge].append(np.mean(scores[judge][prop]))\n",
        "\n",
        "# Create correlation matrix\n",
        "correlation_matrix = np.zeros((5, 5))\n",
        "p_values = np.zeros((5, 5))\n",
        "\n",
        "for i, judge1 in enumerate(judges):\n",
        "    for j, judge2 in enumerate(judges):\n",
        "        if i != j:\n",
        "            corr, p_val = pearsonr(judge_scores[judge1], judge_scores[judge2])\n",
        "            correlation_matrix[i][j] = corr\n",
        "            p_values[i][j] = p_val\n",
        "        else:\n",
        "            correlation_matrix[i][j] = 1.0\n",
        "\n",
        "# Create DataFrame\n",
        "judge_names = [\"ChatGPT o3-pro\", \"Claude Opus 4\", \"Gemini 2.5 Pro\", \"ChatGPT o3\", \"Grok 3 Think\"]\n",
        "df_corr = pd.DataFrame(correlation_matrix,\n",
        "                      index=[f'{judges[i]} ({judge_names[i]})' for i in range(5)],\n",
        "                      columns=[f'{judges[i]}' for i in range(5)])\n",
        "\n",
        "print(\"\\n2. Inter-Rater Correlation Matrix\")\n",
        "print(\"=\"*50)\n",
        "print(df_corr.round(3).to_string())\n",
        "print(f\"\\nAverage Inter-Rater Correlation: {np.mean(correlation_matrix[correlation_matrix != 1.0]):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Bmf3v6Sh6Pbg",
        "outputId": "ca327fd8-c003-4f8d-947c-cef1eb09267a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. Inter-Rater Correlation Matrix\n",
            "==================================================\n",
            "                        A      B      C      D      E\n",
            "A (ChatGPT o3-pro)  1.000  0.967  0.961  0.961  0.911\n",
            "B (Claude Opus 4)   0.967  1.000  0.875  0.864  0.951\n",
            "C (Gemini 2.5 Pro)  0.961  0.875  1.000  0.988  0.760\n",
            "D (ChatGPT o3)      0.961  0.864  0.988  1.000  0.784\n",
            "E (Grok 3 Think)    0.911  0.951  0.760  0.784  1.000\n",
            "\n",
            "Average Inter-Rater Correlation: 0.902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import kendalltau\n",
        "\n",
        "# Calculate rankings for each judge\n",
        "rankings = {}\n",
        "for judge in judges:\n",
        "    judge_totals = [(prop, np.mean(scores[judge][prop])) for prop in ['A', 'B', 'C', 'D', 'E']]\n",
        "    judge_totals.sort(key=lambda x: x[1], reverse=True)\n",
        "    rankings[judge] = [prop for prop, _ in judge_totals]\n",
        "\n",
        "# Calculate Kendall's tau for consensus\n",
        "consensus_data = []\n",
        "judge_pairs = [(i, j) for i in range(5) for j in range(i+1, 5)]\n",
        "\n",
        "for i, j in judge_pairs:\n",
        "    judge1, judge2 = judges[i], judges[j]\n",
        "    tau, p_value = kendalltau([rankings[judge1].index(prop) for prop in ['A', 'B', 'C', 'D', 'E']],\n",
        "                             [rankings[judge2].index(prop) for prop in ['A', 'B', 'C', 'D', 'E']])\n",
        "    consensus_data.append({\n",
        "        'Judge Pair': f'{judge1} vs {judge2}',\n",
        "        'Kendall Tau': round(tau, 3),\n",
        "        'P-value': round(p_value, 3),\n",
        "        'Agreement': 'Strong' if abs(tau) > 0.7 else 'Moderate' if abs(tau) > 0.4 else 'Weak'\n",
        "    })\n",
        "\n",
        "# Create ranking table\n",
        "ranking_df = pd.DataFrame(rankings)\n",
        "ranking_df.index = ['1st', '2nd', '3rd', '4th', '5th']\n",
        "\n",
        "print(\"\\n3. Ranking Consensus Table\")\n",
        "print(\"=\"*50)\n",
        "print(\"Rankings by Judge:\")\n",
        "print(ranking_df.to_string())\n",
        "print(\"\\nPairwise Agreement:\")\n",
        "print(pd.DataFrame(consensus_data).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zlc_lOrx6PhU",
        "outputId": "8f51f424-e184-4e6a-b50e-6bc0f6fef2d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Ranking Consensus Table\n",
            "==================================================\n",
            "Rankings by Judge:\n",
            "     A  B  C  D  E\n",
            "1st  A  B  D  A  B\n",
            "2nd  B  A  A  D  A\n",
            "3rd  D  D  B  B  C\n",
            "4th  C  C  C  C  D\n",
            "5th  E  E  E  E  E\n",
            "\n",
            "Pairwise Agreement:\n",
            "Judge Pair  Kendall Tau  P-value Agreement\n",
            "    A vs B          0.8    0.083    Strong\n",
            "    A vs C          0.6    0.233  Moderate\n",
            "    A vs D          0.8    0.083    Strong\n",
            "    A vs E          0.6    0.233  Moderate\n",
            "    B vs C          0.4    0.483      Weak\n",
            "    B vs D          0.6    0.233  Moderate\n",
            "    B vs E          0.8    0.083    Strong\n",
            "    C vs D          0.8    0.083    Strong\n",
            "    C vs E          0.2    0.817      Weak\n",
            "    D vs E          0.4    0.483      Weak\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Calculate severity/leniency metrics\n",
        "severity_data = []\n",
        "for judge in judges:\n",
        "    all_scores = []\n",
        "    for prop in ['A', 'B', 'C', 'D', 'E']:\n",
        "        all_scores.extend(scores[judge][prop])\n",
        "\n",
        "    avg_score = np.mean(all_scores)\n",
        "    std_score = np.std(all_scores)\n",
        "    min_score = np.min(all_scores)\n",
        "    max_score = np.max(all_scores)\n",
        "    range_score = max_score - min_score\n",
        "\n",
        "    severity_data.append({\n",
        "        'Judge': f'{judge} ({judge_names[ord(judge)-ord(\"A\")]})',\n",
        "        'Average Score': round(avg_score, 2),\n",
        "        'Std Deviation': round(std_score, 2),\n",
        "        'Score Range': round(range_score, 2),\n",
        "        'Min Score': round(min_score, 1),\n",
        "        'Max Score': round(max_score, 1),\n",
        "        'Tendency': 'Lenient' if avg_score > 8.0 else 'Harsh' if avg_score < 7.5 else 'Moderate'\n",
        "    })\n",
        "\n",
        "df_severity = pd.DataFrame(severity_data)\n",
        "print(\"\\n4. Judge Severity/Leniency Index\")\n",
        "print(\"=\"*50)\n",
        "print(df_severity.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "93-7dd7Y6Pl6",
        "outputId": "86c6dd5b-a928-41e9-8ab0-42de92f42f47"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4. Judge Severity/Leniency Index\n",
            "==================================================\n",
            "             Judge  Average Score  Std Deviation  Score Range  Min Score  Max Score Tendency\n",
            "A (ChatGPT o3-pro)           8.05           1.23          4.0        5.5        9.5  Lenient\n",
            " B (Claude Opus 4)           7.65           1.35          4.5        5.0        9.5 Moderate\n",
            "C (Gemini 2.5 Pro)           7.98           1.83          5.0        5.0       10.0 Moderate\n",
            "    D (ChatGPT o3)           7.89           1.20          3.9        5.6        9.5 Moderate\n",
            "  E (Grok 3 Think)           8.35           0.81          2.5        7.0        9.5  Lenient\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Calculate discrimination power for each criterion\n",
        "discrimination_data = []\n",
        "\n",
        "for c_idx, criterion in enumerate(criteria):\n",
        "    criterion_scores = []\n",
        "    for judge in judges:\n",
        "        for prop in ['A', 'B', 'C', 'D', 'E']:\n",
        "            criterion_scores.append(scores[judge][prop][c_idx])\n",
        "\n",
        "    variance = np.var(criterion_scores)\n",
        "    std_dev = np.std(criterion_scores)\n",
        "    score_range = max(criterion_scores) - min(criterion_scores)\n",
        "    mean_score = np.mean(criterion_scores)\n",
        "\n",
        "    discrimination_data.append({\n",
        "        'Criterion': criterion,\n",
        "        'Mean Score': round(mean_score, 2),\n",
        "        'Std Deviation': round(std_dev, 2),\n",
        "        'Variance': round(variance, 2),\n",
        "        'Score Range': round(score_range, 2),\n",
        "        'Discrimination Power': 'High' if std_dev > 1.2 else 'Medium' if std_dev > 0.8 else 'Low'\n",
        "    })\n",
        "\n",
        "df_discrimination = pd.DataFrame(discrimination_data)\n",
        "print(\"\\n5. Criteria Discrimination Power Table\")\n",
        "print(\"=\"*50)\n",
        "print(df_discrimination.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LlDrHq-x6PrC",
        "outputId": "d7b66da0-4613-403a-a6d0-e3849a5e4f8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "5. Criteria Discrimination Power Table\n",
            "==================================================\n",
            "             Criterion  Mean Score  Std Deviation  Variance  Score Range Discrimination Power\n",
            "       Report/MA Cited        8.04           1.34      1.79          4.8                 High\n",
            "Deliverable Completion        8.33           1.16      1.34          4.0               Medium\n",
            "     PDAC Trial Impact        7.93           1.36      1.85          5.0                 High\n",
            "   Funding Probability        7.63           1.42      2.01          4.8                 High\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Calculate summary statistics for each proposal\n",
        "summary_data = []\n",
        "\n",
        "for prop in ['A', 'B', 'C', 'D', 'E']:\n",
        "    all_scores = []\n",
        "    criterion_averages = []\n",
        "\n",
        "    for c_idx in range(4):\n",
        "        criterion_scores = [scores[judge][prop][c_idx] for judge in judges]\n",
        "        criterion_averages.append(np.mean(criterion_scores))\n",
        "        all_scores.extend(criterion_scores)\n",
        "\n",
        "    overall_avg = np.mean(all_scores)\n",
        "    overall_std = np.std(all_scores)\n",
        "\n",
        "    summary_data.append({\n",
        "        'Proposal': f'{prop} ({[\"ChatGPT o3-pro\", \"Claude Opus 4\", \"Gemini 2.5 Pro\", \"ChatGPT o3\", \"Grok 3 Think\"][ord(prop)-ord(\"A\")]})',\n",
        "        'Overall Average': round(overall_avg, 2),\n",
        "        'Overall Std Dev': round(overall_std, 2),\n",
        "        'Report/MA Cited': round(criterion_averages[0], 2),\n",
        "        'Deliverable Completion': round(criterion_averages[1], 2),\n",
        "        'PDAC Trial Impact': round(criterion_averages[2], 2),\n",
        "        'Funding Probability': round(criterion_averages[3], 2),\n",
        "        'Best Criterion': criteria[np.argmax(criterion_averages)],\n",
        "        'Weakest Criterion': criteria[np.argmin(criterion_averages)]\n",
        "    })\n",
        "\n",
        "df_summary = pd.DataFrame(summary_data)\n",
        "print(\"\\n6. Proposal Performance Summary Matrix\")\n",
        "print(\"=\"*50)\n",
        "print(df_summary.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-HCWCRb-6Pvz",
        "outputId": "cb0fe954-9cb9-4398-958c-9b98ee5036a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "6. Proposal Performance Summary Matrix\n",
            "==================================================\n",
            "          Proposal  Overall Average  Overall Std Dev  Report/MA Cited  Deliverable Completion  PDAC Trial Impact  Funding Probability         Best Criterion   Weakest Criterion\n",
            "A (ChatGPT o3-pro)             9.09             0.49             9.20                    9.60               8.90                 8.66 Deliverable Completion Funding Probability\n",
            " B (Claude Opus 4)             8.78             0.48             8.98                    8.86               8.70                 8.58        Report/MA Cited Funding Probability\n",
            "C (Gemini 2.5 Pro)             7.36             0.81             7.46                    8.02               7.22                 6.74 Deliverable Completion Funding Probability\n",
            "    D (ChatGPT o3)             8.74             0.73             8.66                    8.80               8.94                 8.54      PDAC Trial Impact Funding Probability\n",
            "  E (Grok 3 Think)             5.96             0.68             5.92                    6.38               5.90                 5.62 Deliverable Completion Funding Probability\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "\n",
        "# Calculate pairwise comparisons\n",
        "comparison_data = []\n",
        "proposals = ['A', 'B', 'C', 'D', 'E']\n",
        "\n",
        "for prop1, prop2 in combinations(proposals, 2):\n",
        "    wins_prop1 = 0\n",
        "    wins_prop2 = 0\n",
        "    ties = 0\n",
        "\n",
        "    for judge in judges:\n",
        "        score1 = np.mean(scores[judge][prop1])\n",
        "        score2 = np.mean(scores[judge][prop2])\n",
        "\n",
        "        if score1 > score2:\n",
        "            wins_prop1 += 1\n",
        "        elif score2 > score1:\n",
        "            wins_prop2 += 1\n",
        "        else:\n",
        "            ties += 1\n",
        "\n",
        "    # Calculate average score difference\n",
        "    avg_diff = np.mean([np.mean(scores[judge][prop1]) - np.mean(scores[judge][prop2]) for judge in judges])\n",
        "\n",
        "    comparison_data.append({\n",
        "        'Comparison': f'{prop1} vs {prop2}',\n",
        "        f'{prop1} Wins': wins_prop1,\n",
        "        f'{prop2} Wins': wins_prop2,\n",
        "        'Ties': ties,\n",
        "        'Winner': prop1 if wins_prop1 > wins_prop2 else prop2 if wins_prop2 > wins_prop1 else 'Tie',\n",
        "        'Avg Score Difference': round(abs(avg_diff), 2),\n",
        "        'Consensus': 'Strong' if max(wins_prop1, wins_prop2) >= 4 else 'Moderate' if max(wins_prop1, wins_prop2) >= 3 else 'Weak'\n",
        "    })\n",
        "\n",
        "df_pairwise = pd.DataFrame(comparison_data)\n",
        "print(\"\\n7. Pairwise Proposal Comparison Table\")\n",
        "print(\"=\"*50)\n",
        "print(df_pairwise.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5KrBeB8S6P0n",
        "outputId": "bd17dedc-a524-46c6-e9f0-f1afc81d5348"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "7. Pairwise Proposal Comparison Table\n",
            "==================================================\n",
            "Comparison  A Wins  B Wins  Ties Winner  Avg Score Difference Consensus  C Wins  D Wins  E Wins\n",
            "    A vs B     3.0     2.0     0      A                  0.31  Moderate     NaN     NaN     NaN\n",
            "    A vs C     5.0     NaN     0      A                  1.73    Strong     0.0     NaN     NaN\n",
            "    A vs D     4.0     NaN     0      A                  0.35    Strong     NaN     1.0     NaN\n",
            "    A vs E     5.0     NaN     0      A                  3.14    Strong     NaN     NaN     0.0\n",
            "    B vs C     NaN     5.0     0      B                  1.42    Strong     0.0     NaN     NaN\n",
            "    B vs D     NaN     3.0     0      B                  0.04  Moderate     NaN     2.0     NaN\n",
            "    B vs E     NaN     5.0     0      B                  2.82    Strong     NaN     NaN     0.0\n",
            "    C vs D     NaN     NaN     0      D                  1.38    Strong     1.0     4.0     NaN\n",
            "    C vs E     NaN     NaN     0      C                  1.40    Strong     5.0     NaN     0.0\n",
            "    D vs E     NaN     NaN     0      D                  2.78    Strong     NaN     5.0     0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Collect all scores and create frequency distribution\n",
        "all_scores = []\n",
        "for judge in judges:\n",
        "    for prop in ['A', 'B', 'C', 'D', 'E']:\n",
        "        all_scores.extend(scores[judge][prop])\n",
        "\n",
        "# Create score bins\n",
        "score_bins = [(5.0, 6.0), (6.0, 7.0), (7.0, 8.0), (8.0, 9.0), (9.0, 10.0), (10.0, 10.1)]\n",
        "bin_labels = ['5.0-5.9', '6.0-6.9', '7.0-7.9', '8.0-8.9', '9.0-9.9', '10.0']\n",
        "\n",
        "distribution_data = []\n",
        "for i, (lower, upper) in enumerate(score_bins):\n",
        "    count = sum(1 for score in all_scores if lower <= score < upper)\n",
        "    percentage = (count / len(all_scores)) * 100\n",
        "\n",
        "    distribution_data.append({\n",
        "        'Score Range': bin_labels[i],\n",
        "        'Frequency': count,\n",
        "        'Percentage': round(percentage, 1),\n",
        "        'Cumulative %': round(sum(d['Percentage'] for d in distribution_data) + percentage, 1)\n",
        "    })\n",
        "\n",
        "df_distribution = pd.DataFrame(distribution_data)\n",
        "print(\"\\n8. Score Distribution Frequency Table\")\n",
        "print(\"=\"*50)\n",
        "print(df_distribution.to_string(index=False))\n",
        "print(f\"\\nTotal Scores: {len(all_scores)}\")\n",
        "print(f\"Mean Score: {np.mean(all_scores):.2f}\")\n",
        "print(f\"Median Score: {np.median(all_scores):.2f}\")\n",
        "print(f\"Standard Deviation: {np.std(all_scores):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xsrsWx9C6P5g",
        "outputId": "e9e151e8-8865-4c2d-a081-c96a9ac902f1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "8. Score Distribution Frequency Table\n",
            "==================================================\n",
            "Score Range  Frequency  Percentage  Cumulative %\n",
            "    5.0-5.9          9         9.0           9.0\n",
            "    6.0-6.9         11        11.0          20.0\n",
            "    7.0-7.9         16        16.0          36.0\n",
            "    8.0-8.9         28        28.0          64.0\n",
            "    9.0-9.9         34        34.0          98.0\n",
            "       10.0          2         2.0         100.0\n",
            "\n",
            "Total Scores: 100\n",
            "Mean Score: 7.98\n",
            "Median Score: 8.45\n",
            "Standard Deviation: 1.35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Prepare criteria scores\n",
        "criteria_scores = {criterion: [] for criterion in criteria}\n",
        "\n",
        "for judge in judges:\n",
        "    for prop in ['A', 'B', 'C', 'D', 'E']:\n",
        "        for c_idx, criterion in enumerate(criteria):\n",
        "            criteria_scores[criterion].append(scores[judge][prop][c_idx])\n",
        "\n",
        "# Calculate correlation matrix between criteria\n",
        "correlation_data = []\n",
        "for i, crit1 in enumerate(criteria):\n",
        "    for j, crit2 in enumerate(criteria):\n",
        "        if i <= j:  # Only upper triangle and diagonal\n",
        "            if i == j:\n",
        "                correlation_data.append({\n",
        "                    'Criterion 1': crit1,\n",
        "                    'Criterion 2': crit2,\n",
        "                    'Correlation': 1.000,\n",
        "                    'P-value': 0.000,\n",
        "                    'Relationship': 'Perfect'\n",
        "                })\n",
        "            else:\n",
        "                corr, p_val = pearsonr(criteria_scores[crit1], criteria_scores[crit2])\n",
        "                relationship = 'Strong' if abs(corr) > 0.7 else 'Moderate' if abs(corr) > 0.4 else 'Weak'\n",
        "\n",
        "                correlation_data.append({\n",
        "                    'Criterion 1': crit1,\n",
        "                    'Criterion 2': crit2,\n",
        "                    'Correlation': round(corr, 3),\n",
        "                    'P-value': round(p_val, 3),\n",
        "                    'Relationship': relationship\n",
        "                })\n",
        "\n",
        "df_cross_corr = pd.DataFrame(correlation_data)\n",
        "print(\"\\n9. Cross-Criteria Correlation Analysis\")\n",
        "print(\"=\"*50)\n",
        "print(df_cross_corr.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IHgmrk6m6P-g",
        "outputId": "769cdc0e-0dec-489a-f5fd-a40361fa4edf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "9. Cross-Criteria Correlation Analysis\n",
            "==================================================\n",
            "           Criterion 1            Criterion 2  Correlation  P-value Relationship\n",
            "       Report/MA Cited        Report/MA Cited        1.000      0.0      Perfect\n",
            "       Report/MA Cited Deliverable Completion        0.950      0.0       Strong\n",
            "       Report/MA Cited      PDAC Trial Impact        0.965      0.0       Strong\n",
            "       Report/MA Cited    Funding Probability        0.937      0.0       Strong\n",
            "Deliverable Completion Deliverable Completion        1.000      0.0      Perfect\n",
            "Deliverable Completion      PDAC Trial Impact        0.898      0.0       Strong\n",
            "Deliverable Completion    Funding Probability        0.857      0.0       Strong\n",
            "     PDAC Trial Impact      PDAC Trial Impact        1.000      0.0      Perfect\n",
            "     PDAC Trial Impact    Funding Probability        0.934      0.0       Strong\n",
            "   Funding Probability    Funding Probability        1.000      0.0      Perfect\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import f_oneway, ttest_ind\n",
        "\n",
        "# ANOVA test for proposal differences\n",
        "proposal_scores = {}\n",
        "for prop in ['A', 'B', 'C', 'D', 'E']:\n",
        "    proposal_scores[prop] = []\n",
        "    for judge in judges:\n",
        "        proposal_scores[prop].append(np.mean(scores[judge][prop]))\n",
        "\n",
        "# One-way ANOVA\n",
        "f_stat, p_value_anova = f_oneway(*[proposal_scores[prop] for prop in ['A', 'B', 'C', 'D', 'E']])\n",
        "\n",
        "# Pairwise t-tests between top proposals\n",
        "significance_data = []\n",
        "\n",
        "# ANOVA result\n",
        "significance_data.append({\n",
        "    'Test': 'One-way ANOVA (All Proposals)',\n",
        "    'Test Statistic': round(f_stat, 3),\n",
        "    'P-value': round(p_value_anova, 4),\n",
        "    'Significant': 'Yes' if p_value_anova < 0.05 else 'No',\n",
        "    'Interpretation': 'Significant differences exist between proposals' if p_value_anova < 0.05 else 'No significant differences'\n",
        "})\n",
        "\n",
        "# Top proposals comparison\n",
        "top_proposals = ['A', 'B', 'D']  # Based on general performance\n",
        "for i, prop1 in enumerate(top_proposals):\n",
        "    for prop2 in top_proposals[i+1:]:\n",
        "        t_stat, p_val = ttest_ind(proposal_scores[prop1], proposal_scores[prop2])\n",
        "        significance_data.append({\n",
        "            'Test': f'{prop1} vs {prop2} (t-test)',\n",
        "            'Test Statistic': round(t_stat, 3),\n",
        "            'P-value': round(p_val, 4),\n",
        "            'Significant': 'Yes' if p_val < 0.05 else 'No',\n",
        "            'Interpretation': f'{\"Significant\" if p_val < 0.05 else \"No significant\"} difference between {prop1} and {prop2}'\n",
        "        })\n",
        "\n",
        "df_significance = pd.DataFrame(significance_data)\n",
        "print(\"\\n10. Statistical Significance Testing Table\")\n",
        "print(\"=\"*50)\n",
        "print(df_significance.to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PoLFRaVY6QDX",
        "outputId": "cb9e4c50-4ca6-4068-993e-e87bc747b220"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "10. Statistical Significance Testing Table\n",
            "==================================================\n",
            "                         Test  Test Statistic  P-value Significant                                  Interpretation\n",
            "One-way ANOVA (All Proposals)          27.285   0.0000         Yes Significant differences exist between proposals\n",
            "              A vs B (t-test)           1.338   0.2177          No       No significant difference between A and B\n",
            "              A vs D (t-test)           1.049   0.3248          No       No significant difference between A and D\n",
            "              B vs D (t-test)           0.126   0.9029          No       No significant difference between B and D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Calculate averages for each proposal across all judges for each criterion\n",
        "average_data = []\n",
        "\n",
        "for prop in ['A', 'B', 'C', 'D', 'E']:\n",
        "    row_data = {'Proposal': f'{prop} ({judge_names[ord(prop)-ord(\"A\")]})'}\n",
        "\n",
        "    for c_idx, criterion in enumerate(criteria):\n",
        "        criterion_scores = [scores[judge][prop][c_idx] for judge in judges]\n",
        "        avg_score = np.mean(criterion_scores)\n",
        "        row_data[criterion] = round(avg_score, 2)\n",
        "\n",
        "    # Calculate overall average\n",
        "    all_scores = []\n",
        "    for judge in judges:\n",
        "        all_scores.extend(scores[judge][prop])\n",
        "    row_data['Overall Average'] = round(np.mean(all_scores), 2)\n",
        "\n",
        "    average_data.append(row_data)\n",
        "\n",
        "df_averages = pd.DataFrame(average_data)\n",
        "print(\"\\n11. Average Scores Table (All Judges)\")\n",
        "print(\"=\"*50)\n",
        "print(df_averages.to_string(index=False))\n",
        "\n",
        "# Add ranking based on overall average\n",
        "df_averages_sorted = df_averages.sort_values('Overall Average', ascending=False)\n",
        "df_averages_sorted['Rank'] = range(1, len(df_averages_sorted) + 1)\n",
        "print(\"\\n\\nRanked by Overall Average:\")\n",
        "print(df_averages_sorted[['Rank', 'Proposal', 'Overall Average']].to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "E_77vpy06QIY",
        "outputId": "df0a8730-9631-4dcf-a0b0-bdad38ef4caf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "11. Average Scores Table (All Judges)\n",
            "==================================================\n",
            "          Proposal  Report/MA Cited  Deliverable Completion  PDAC Trial Impact  Funding Probability  Overall Average\n",
            "A (ChatGPT o3-pro)             9.20                    9.60               8.90                 8.66             9.09\n",
            " B (Claude Opus 4)             8.98                    8.86               8.70                 8.58             8.78\n",
            "C (Gemini 2.5 Pro)             7.46                    8.02               7.22                 6.74             7.36\n",
            "    D (ChatGPT o3)             8.66                    8.80               8.94                 8.54             8.74\n",
            "  E (Grok 3 Think)             5.92                    6.38               5.90                 5.62             5.96\n",
            "\n",
            "\n",
            "Ranked by Overall Average:\n",
            " Rank           Proposal  Overall Average\n",
            "    1 A (ChatGPT o3-pro)             9.09\n",
            "    2  B (Claude Opus 4)             8.78\n",
            "    3     D (ChatGPT o3)             8.74\n",
            "    4 C (Gemini 2.5 Pro)             7.36\n",
            "    5   E (Grok 3 Think)             5.96\n"
          ]
        }
      ]
    }
  ]
}